{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of DDPG algorithm employed\n",
    "\n",
    "This script is an example of the Deep Deterministic Policy Gradient (DDPG) algorithm, which is a type of reinforcement learning strategy. \n",
    "\n",
    "The DDPG algorithm uses two neural networks: an actor network that decides what actions to take, and a critic network that determines how good those actions are.\n",
    "\n",
    "1. The Actor (Low Bias, High Variance): The actor is trying to directly learn the optimal policy and thus has low bias. However, the target it's trying to learn from - the critic's estimation of the action value function - can be noisy and have high variance, especially early in training when the critic itself is still learning.\n",
    "\n",
    "2. The Critic (High Bias, Low Variance): The critic is trying to estimate the true action value function based on the current policy of the actor. It's an approximation and thus can suffer from bias. However, because it uses the Bellman equation for updates - averaging over next state values - this can help to reduce variance. \n",
    "(Note. it uses Temporal Difference (TD) learning to estimate the Q-value (action-value) function. TD learning makes use of the Bellman equation to estimate the value of a state or action, which is inherently an approximation because it bootstraps from other estimates.)\n",
    "\n",
    "Together, the interaction of the actor and critic can help balance out these considerations, with the actor providing direction for improvement and the critic correcting for errors in these directions based on its value estimations.\n",
    "\n",
    "##### Process Summary\n",
    "\n",
    "1. The environment is initialized along with the DDPG agents.\n",
    "2. For each episode, the environment is reset and the agent selects an action based on its current policy (the actor network).\n",
    "3. The environment executes the action and returns the next state, reward, and whether the episode has ended.\n",
    "4. This information is added to a replay buffer, a memory structure used to store past experiences.\n",
    "5. Periodically, the agent samples a batch of experiences from the replay buffer and uses it to update the policy (actor network) and the value function (critic network).\n",
    "6. The actor network is updated to maximize the expected return as evaluated by the critic network. The critic network is updated to minimize the difference between its prediction of the return and the actual return.\n",
    "7. The weights of the actor and critic networks are softly updated to their target counterparts, which stabilizes learning by allowing the targets to change slowly.\n",
    "8. This loop continues until termination.\n",
    "\n",
    "#### Goal\n",
    "\n",
    "The goal of this script in particular is to train agents to perform a predifined goal over 100 episodes. When that goal is reached, the trained model parameters are saved for future use.\n",
    "\n",
    "#### Content\n",
    "\n",
    "In the following sections, there are: \n",
    "- Random model: one model of random learning not achieving result (commented out)\n",
    "- One-agent model: model with ddpg implemented with one agent but slow in achieving results (commented out)\n",
    "- Multi-agent model: the last model is the one I selected to achieve the desired result\n",
    "\n",
    "#### Last notes\n",
    "\n",
    "- It's required to restart the kernel one more time to be sure packages are installed well (eg. pytorch)\n",
    "- Weights are saved in appropriate pth files for reproducibility purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \\nstates = env_info.vector_observations                  # get the current state (for each agent)\\nscores = np.zeros(num_agents)                          # initialize the score (for each agent)\\nwhile True:\\n    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\\n    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\\n    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\\n    next_states = env_info.vector_observations         # get next state (for each agent)\\n    rewards = env_info.rewards                         # get reward (for each agent)\\n    dones = env_info.local_done                        # see if episode finished\\n    scores += env_info.rewards                         # update the score (for each agent)\\n    states = next_states                               # roll over states to next time step\\n    if np.any(dones):                                  # exit loop if episode finished\\n        break\\nprint('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Commented out\n",
    "'''env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script isympy is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport random\\nfrom collections import namedtuple, deque\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\n\\nclass Actor(nn.Module):\\n    \"\"\"Actor (Policy) Model.\"\"\"\\n\\n    def __init__(self, state_size, action_size, seed):\\n\\n        super(Actor, self).__init__()\\n        self.seed = torch.manual_seed(seed)\\n\\n        self.bn0 = nn.BatchNorm1d(state_size)\\n        self.fc1 = nn.Linear(state_size, 128)\\n        self.bn1 = nn.BatchNorm1d(128)\\n        self.fc2 = nn.Linear(128, 64)\\n        self.bn2 = nn.BatchNorm1d(64)\\n        self.fc3 = nn.Linear(64, 32)\\n        self.bn3 = nn.BatchNorm1d(32)\\n        self.fc4 = nn.Linear(32, action_size)\\n\\n\\n    def forward(self, state):\\n        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\\n        x = self.bn0(state)\\n        x = F.selu(self.bn1(self.fc1(x)))\\n        x = F.selu(self.bn2(self.fc2(x)))\\n        x = F.selu(self.bn3(self.fc3(x)))\\n        return torch.tanh(self.fc4(x))\\n\\n\\nclass Critic(nn.Module):\\n    def __init__(self, state_size, action_size, seed):\\n\\n        super(Critic, self).__init__()\\n        self.seed = torch.manual_seed(seed)\\n        \\n        self.bn0 = nn.BatchNorm1d(state_size)\\n        self.fcs1 = nn.Linear(state_size, 128)\\n        self.fc2 = nn.Linear(128 + action_size, 64)\\n        self.fc3 = nn.Linear(64, 32)\\n        self.fc4 = nn.Linear(32, 16)\\n        self.fc5 = nn.Linear(16, 1)\\n\\n\\n    def forward(self, state, action):\\n        state = self.bn0(state)\\n        x_state = F.selu(self.fcs1(state))\\n        x = torch.cat((x_state, action), dim=1)\\n        x = F.selu(self.fc2(x))\\n        x = F.selu(self.fc3(x))\\n        x = F.selu(self.fc4(x))\\n        return  F.selu(self.fc5(x))\\n\\n\\nclass OUNoise:\\n    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):\\n        self.size = size\\n        self.mu = mu * np.ones(size)\\n        self.theta = theta\\n        self.sigma = sigma\\n        self.reset()\\n\\n    def reset(self):\\n        self.state = np.copy(self.mu)\\n\\n    def sample(self):\\n        x = self.state\\n        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\\n        self.state = x + dx\\n        return self.state\\n\\n\\n# Replay Buffer Size\\nBUFFER_SIZE = int(1e6)\\n# Minibatch Size\\nBATCH_SIZE = 256\\n# Discount Gamma\\nGAMMA = 0.995 \\n# Soft Update Value\\nTAU = 1e-2   \\n# Learning rates for each NN      \\nLR_ACTOR = 1e-3 \\nLR_CRITIC = 1e-3\\n# Update network every X timesteps\\nUPDATE_EVERY = 32\\n# Learn from batch of experiences n_experiences times\\nN_EXPERIENCES = 16   \\n\\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\n#device = torch.device(\"cpu\")\\n\\nclass DDPGAgent():\\n    \"\"\"Interacts with and learns from the environment using the DDPG algorithm.\"\"\"\\n\\n    def __init__(self, state_size, action_size, random_seed):\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.seed = random.seed(random_seed)\\n\\n        # Actor Neural Network (Regular and target)\\n        self.actor_regular = Actor(state_size, action_size, random_seed).to(device)\\n        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\\n        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=LR_ACTOR)\\n\\n        # Critic Neural Network (Regular and target)\\n        self.critic_regular = Critic(state_size, action_size, random_seed).to(device)\\n        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\\n        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=LR_CRITIC)\\n\\n        # Replay memory\\n        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\\n          \\n        # Ensure that both networks have the same weights\\n        self.deep_copy(self.actor_target, self.actor_regular)\\n        self.deep_copy(self.critic_target, self.critic_regular)\\n\\n    def step(self, states, actions, rewards, next_states, dones, timestep):\\n        # Save collected experiences\\n        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\\n            self.memory.add(state, action, reward, next_state, done)\\n\\n        # Learn from our buffer if possible\\n        if len(self.memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\\n            for _ in range(N_EXPERIENCES):\\n                experiences = self.memory.sample()\\n                self.learn(experiences, GAMMA)\\n\\n    def act(self, states):\\n        states = torch.from_numpy(states).float().to(device)\\n        \\n        # Evaluation mode\\n        # Notify all your layers that you are in eval mode, that way, \\n        # Batchnorm or dropout layers will work in eval mode instead of training mode.\\n        self.actor_regular.eval()\\n        # torch.no_grad() impacts the autograd engine and deactivate it. \\n        # It will reduce memory usage and speed up\\n        with torch.no_grad():\\n            actions = self.actor_regular(states).cpu().data.numpy()\\n        # Enable Training mode\\n        self.actor_regular.train()\\n\\n        return actions\\n\\n\\n    def learn(self, experiences, gamma):\\n        states, actions, rewards, next_states, dones = experiences\\n        \\n        #--------------------------------\\n        # Update the critic neural network\\n        #--------------------------------\\n        \\n        # Get predicted next-state actions\\n        actions_next = self.actor_target(next_states)\\n        #Get Q values from target model\\n        Q_targets_next = self.critic_target(next_states, actions_next)\\n\\n        # Compute Q targets for current states\\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\\n\\n        # Calculate the critic loss\\n        Q_expected = self.critic_regular(states, actions)\\n        critic_loss = F.mse_loss(Q_expected, Q_targets)\\n\\n        # Minimize the loss\\n        self.critic_optimizer.zero_grad()\\n        critic_loss.backward()\\n        self.critic_optimizer.step()\\n        \\n        #--------------------------------\\n        # Update the actor neural network\\n        #--------------------------------\\n        \\n        # Calculate the actor loss\\n        actions_pred = self.actor_regular(states)\\n        # Change sign because of the gradient descent\\n        actor_loss = -self.critic_regular(states, actions_pred).mean()\\n\\n        # Minimize the loss function\\n        self.actor_optimizer.zero_grad()\\n        actor_loss.backward()\\n        self.actor_optimizer.step()\\n\\n        # Update target network using the soft update approach (slowly updating)\\n        self.soft_update(self.critic_regular, self.critic_target, TAU)\\n        self.soft_update(self.actor_regular, self.actor_target, TAU)\\n\\n\\n    def soft_update(self, regular_model, target_model, tau):\\n        # Update the target network slowly to improve the stability\\n        for target_param, regular_param in zip(target_model.parameters(), regular_model.parameters()):\\n            target_param.data.copy_(tau*regular_param.data + (1.0-tau) * target_param.data)\\n\\n    def deep_copy(self, target, source):\\n        for target_param, param in zip(target.parameters(), source.parameters()):\\n            target_param.data.copy_(param.data)\\n\\n\\nclass ReplayBuffer:\\n    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\\n    \\n    def __init__(self, action_size, buffer_size, batch_size, seed):\\n        \"\"\"Initialize a ReplayBuffer object.\\n        Params\\n        ======\\n            buffer_size (int): maximum size of buffer\\n            batch_size (int): size of each training batch\\n        \"\"\"\\n        self.action_size = action_size\\n        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\\n        self.batch_size = batch_size\\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\\n        self.seed = random.seed(seed)\\n\\n    def add(self, state, action, reward, next_state, done):\\n        \"\"\"Add a new experience to memory.\"\"\"\\n        e = self.experience(state, action, reward, next_state, done)\\n        self.memory.append(e)\\n\\n    def sample(self):\\n        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\\n        experiences = random.sample(self.memory, k=self.batch_size)\\n\\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\\n\\n        return (states, actions, rewards, next_states, dones)\\n\\n    def __len__(self):\\n        \"\"\"Return the current size of internal memory.\"\"\"\\n        return len(self.memory)\\n\\n\\n\\n# Environment Goal\\nGOAL = 30.1\\n# Averaged score\\nSCORE_AVERAGED = 100\\n# Let us know the progress each 10 timesteps\\nPRINT_EVERY = 10\\n# Number of episode for training\\nN_EPISODES = 500\\n# Max Timesteps\\nMAX_TIMESTEPS = 1000\\n\\nn_agents = 1\\n\\n# Initialize environment and agent\\nagent = DDPGAgent(state_size, action_size, random_seed=1)\\n\\n#  Method for training the agent\\ndef train(n_episodes=N_EPISODES):\\n    scores_deque = deque(maxlen=SCORE_AVERAGED)\\n    global_scores = []\\n    averaged_scores = []\\n    \\n    for episode in range(1, N_EPISODES + 1):\\n        states = env.reset(train_mode=True)[brain_name].vector_observations # Get the current states for each agent\\n        scores = np.zeros(n_agents) # Init the score of each agent to zeros                \\n\\n        for t in range(MAX_TIMESTEPS):\\n            actions = agent.act(states) # Act according to our policy\\n            env_info = env.step(actions)[brain_name] # Send the decided actions to all the agents       \\n            next_states = env_info.vector_observations # Get next state for each agent\\n            rewards = env_info.rewards # Get rewards obtained from each agent           \\n            dones = env_info.local_done # Info about if an env is done   \\n            agent.step(states, actions, rewards, next_states, dones, t)  # Learn from the collected experience\\n            states = next_states # Update current states\\n            scores += rewards # Add the rewards recieved  \\n            \\n            # Stop the loop if an agent is done               \\n            if np.any(dones):                          \\n                break\\n                \\n        # Calculate scores and averages\\n        score = np.mean(scores)\\n        scores_deque.append(score)\\n        avg_score = np.mean(scores_deque)\\n        \\n        global_scores.append(score)\\n        averaged_scores.append(avg_score)\\n                \\n        if episode % PRINT_EVERY == 0:\\n            print(\\'\\rEpisode {}\\tAverage Score: {:.2f}\\'.format(episode, np.mean(scores_deque)))  \\n            \\n        if avg_score >= GOAL:  \\n            print(\\'\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\\'.format(episode, avg_score))\\n            torch.save(agent.actor_regular.state_dict(), \\'actor_theta.pth\\')\\n            torch.save(agent.critic_regular.state_dict(), \\'critic_theta.pth\\')\\n            break\\n            \\n    return global_scores, averaged_scores\\n\\n# Train the agent and get the results\\nscores, averages = train()\\n\\n# Plot Statistics (Global scores and averaged scores)\\nplt.subplot(2, 1, 2)\\nplt.plot(np.arange(1, len(scores) + 1), averages)\\nplt.ylabel(\\'Reacher Environment Average Score\\')\\nplt.xlabel(\\'Episode #\\')\\nplt.show()'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc4 = nn.Linear(32, action_size)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = self.bn0(state)\n",
    "        x = F.selu(self.bn1(self.fc1(x)))\n",
    "        x = F.selu(self.bn2(self.fc2(x)))\n",
    "        x = F.selu(self.bn3(self.fc3(x)))\n",
    "        return torch.tanh(self.fc4(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fcs1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128 + action_size, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state = self.bn0(state)\n",
    "        x_state = F.selu(self.fcs1(state))\n",
    "        x = torch.cat((x_state, action), dim=1)\n",
    "        x = F.selu(self.fc2(x))\n",
    "        x = F.selu(self.fc3(x))\n",
    "        x = F.selu(self.fc4(x))\n",
    "        return  F.selu(self.fc5(x))\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "# Replay Buffer Size\n",
    "BUFFER_SIZE = int(1e6)\n",
    "# Minibatch Size\n",
    "BATCH_SIZE = 256\n",
    "# Discount Gamma\n",
    "GAMMA = 0.995 \n",
    "# Soft Update Value\n",
    "TAU = 1e-2   \n",
    "# Learning rates for each NN      \n",
    "LR_ACTOR = 1e-3 \n",
    "LR_CRITIC = 1e-3\n",
    "# Update network every X timesteps\n",
    "UPDATE_EVERY = 32\n",
    "# Learn from batch of experiences n_experiences times\n",
    "N_EXPERIENCES = 16   \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment using the DDPG algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Neural Network (Regular and target)\n",
    "        self.actor_regular = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Neural Network (Regular and target)\n",
    "        self.critic_regular = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "          \n",
    "        # Ensure that both networks have the same weights\n",
    "        self.deep_copy(self.actor_target, self.actor_regular)\n",
    "        self.deep_copy(self.critic_target, self.critic_regular)\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones, timestep):\n",
    "        # Save collected experiences\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn from our buffer if possible\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\n",
    "            for _ in range(N_EXPERIENCES):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        # Evaluation mode\n",
    "        # Notify all your layers that you are in eval mode, that way, \n",
    "        # Batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "        self.actor_regular.eval()\n",
    "        # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "        # It will reduce memory usage and speed up\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_regular(states).cpu().data.numpy()\n",
    "        # Enable Training mode\n",
    "        self.actor_regular.train()\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the critic neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Get predicted next-state actions\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        #Get Q values from target model\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Calculate the critic loss\n",
    "        Q_expected = self.critic_regular(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the actor neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Calculate the actor loss\n",
    "        actions_pred = self.actor_regular(states)\n",
    "        # Change sign because of the gradient descent\n",
    "        actor_loss = -self.critic_regular(states, actions_pred).mean()\n",
    "\n",
    "        # Minimize the loss function\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target network using the soft update approach (slowly updating)\n",
    "        self.soft_update(self.critic_regular, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_regular, self.actor_target, TAU)\n",
    "\n",
    "\n",
    "    def soft_update(self, regular_model, target_model, tau):\n",
    "        # Update the target network slowly to improve the stability\n",
    "        for target_param, regular_param in zip(target_model.parameters(), regular_model.parameters()):\n",
    "            target_param.data.copy_(tau*regular_param.data + (1.0-tau) * target_param.data)\n",
    "\n",
    "    def deep_copy(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "# Environment Goal\n",
    "GOAL = 30.1\n",
    "# Averaged score\n",
    "SCORE_AVERAGED = 100\n",
    "# Let us know the progress each 10 timesteps\n",
    "PRINT_EVERY = 10\n",
    "# Number of episode for training\n",
    "N_EPISODES = 500\n",
    "# Max Timesteps\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "n_agents = 1\n",
    "\n",
    "# Initialize environment and agent\n",
    "agent = DDPGAgent(state_size, action_size, random_seed=1)\n",
    "\n",
    "#  Method for training the agent\n",
    "def train(n_episodes=N_EPISODES):\n",
    "    scores_deque = deque(maxlen=SCORE_AVERAGED)\n",
    "    global_scores = []\n",
    "    averaged_scores = []\n",
    "    \n",
    "    for episode in range(1, N_EPISODES + 1):\n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations # Get the current states for each agent\n",
    "        scores = np.zeros(n_agents) # Init the score of each agent to zeros                \n",
    "\n",
    "        for t in range(MAX_TIMESTEPS):\n",
    "            actions = agent.act(states) # Act according to our policy\n",
    "            env_info = env.step(actions)[brain_name] # Send the decided actions to all the agents       \n",
    "            next_states = env_info.vector_observations # Get next state for each agent\n",
    "            rewards = env_info.rewards # Get rewards obtained from each agent           \n",
    "            dones = env_info.local_done # Info about if an env is done   \n",
    "            agent.step(states, actions, rewards, next_states, dones, t)  # Learn from the collected experience\n",
    "            states = next_states # Update current states\n",
    "            scores += rewards # Add the rewards recieved  \n",
    "            \n",
    "            # Stop the loop if an agent is done               \n",
    "            if np.any(dones):                          \n",
    "                break\n",
    "                \n",
    "        # Calculate scores and averages\n",
    "        score = np.mean(scores)\n",
    "        scores_deque.append(score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        \n",
    "        global_scores.append(score)\n",
    "        averaged_scores.append(avg_score)\n",
    "                \n",
    "        if episode % PRINT_EVERY == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_deque)))  \n",
    "            \n",
    "        if avg_score >= GOAL:  \n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, avg_score))\n",
    "            torch.save(agent.actor_regular.state_dict(), 'actor_theta.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'critic_theta.pth')\n",
    "            break\n",
    "            \n",
    "    return global_scores, averaged_scores\n",
    "\n",
    "# Train the agent and get the results\n",
    "scores, averages = train()\n",
    "\n",
    "# Plot Statistics (Global scores and averaged scores)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.arange(1, len(scores) + 1), averages)\n",
    "plt.ylabel('Reacher Environment Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: 2.43ing average:  2.43\n",
      "Episode 40\tAverage Score: 9.00ing average:  9.00\n",
      "Episode  42 \tScore: 29.30 \tMoving average:  9.87"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def reset_parameters(layers):\n",
    "    for layer in layers:\n",
    "        layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_layers=[400,300]):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc_layers (list): Number of nodes in hidden layers\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Define input and output values for the hidden layers\n",
    "        dims = [state_size] + fc_layers + [action_size]\n",
    "        # Create the hidden layers\n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(dim_in, dim_out) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        # Initialize the hidden layer weights\n",
    "        reset_parameters(self.fc_layers)\n",
    "\n",
    "        #print('Actor network built:', self.fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        # Pass the input through all the layers apllying ReLU activation, but the last\n",
    "        for layer in self.fc_layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Pass the result through the output layer apllying hyperbolic tangent function\n",
    "        x = torch.tanh(self.fc_layers[-1](x))\n",
    "        # Return the better action for the input state\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_layers=[400,300]):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc_layers (list): Number of nodes in hidden layers\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Append the output size to the layers's dimensions\n",
    "        dims = fc_layers + [1]\n",
    "        # Create a list of layers\n",
    "        layers_list = []\n",
    "        layers_list.append(nn.Linear(state_size, dims[0]))\n",
    "        # The second layer receives the the first layer output + action\n",
    "        layers_list.append(nn.Linear(dims[0] + action_size, dims[1]))\n",
    "        # Build the next layers, if that is the case\n",
    "        for dim_in, dim_out in zip(dims[1:-1], dims[2:]):\n",
    "            layers_list.append(nn.Linear(dim_in, dim_out))\n",
    "        # Store the layers as a ModuleList\n",
    "        self.fc_layers = nn.ModuleList(layers_list)\n",
    "        # Initialize the hidden layer weights\n",
    "        reset_parameters(self.fc_layers)\n",
    "        # Add batch normalization to the first hidden layer\n",
    "        self.bn = nn.BatchNorm1d(dims[0])\n",
    "\n",
    "        #print('Critic network built:', self.fc_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        # Pass the states into the first layer\n",
    "        x = self.fc_layers[0](state)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        # Concatenate the first layer output with the action\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        # Pass the input through all the layers apllying ReLU activation, but the last\n",
    "        for layer in self.fc_layers[1:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Pass the result through the output layer apllying sigmoid activation\n",
    "        x = torch.sigmoid(self.fc_layers[-1](x))\n",
    "        # Return the Q-Value for the input state-action\n",
    "        return x\n",
    "\n",
    "# Replay Buffer Size\n",
    "BUFFER_SIZE = int(1e6)\n",
    "# Minibatch Size\n",
    "BATCH_SIZE = 256\n",
    "# Discount Gamma\n",
    "GAMMA = 0.995 \n",
    "# Soft Update Value\n",
    "TAU = 1e-2   \n",
    "# Learning rates for each NN      \n",
    "LR_ACTOR = 1e-3 \n",
    "LR_CRITIC = 1e-3\n",
    "# Update network every X timesteps\n",
    "UPDATE_EVERY = 32\n",
    "# Learn from batch of experiences n_experiences times\n",
    "N_EXPERIENCES = 16   \n",
    "\n",
    "#NOISE_DECAY = 0.999\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment using the DDPG algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Neural Network (Regular and target)\n",
    "        self.actor_regular = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Neural Network (Regular and target)\n",
    "        self.critic_regular = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        # Noise process\n",
    "        #self.noise = OUNoise(action_size, random_seed)\n",
    "        #self.noise_decay = NOISE_DECAY\n",
    "          \n",
    "        # Ensure that both networks have the same weights\n",
    "        self.soft_update(self.actor_target, self.actor_regular, TAU)\n",
    "        self.soft_update(self.critic_target, self.critic_regular, TAU)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        # Save collected experiences\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn from our buffer if possible\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\n",
    "            for _ in range(N_EXPERIENCES):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        # Evaluation mode\n",
    "        # Notify all your layers that you are in eval mode, that way, \n",
    "        # Batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "        self.actor_regular.eval()\n",
    "        # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "        # It will reduce memory usage and speed up\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_regular(states).cpu().data.numpy() #+ self.noise.get_noise()\n",
    "        # Enable Training mode\n",
    "        self.actor_regular.train()\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    #def reset(self):\n",
    "    #    self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the critic neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Get predicted next-state actions\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        #Get Q values from target model\n",
    "        #Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        #Detach tensor from computation graph before performing operations on it:\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next).detach()\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Calculate the critic loss\n",
    "        Q_expected = self.critic_regular(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the actor neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Calculate the actor loss\n",
    "        actions_pred = self.actor_regular(states)\n",
    "        # Change sign because of the gradient descent\n",
    "        actor_loss = -self.critic_regular(states, actions_pred).mean()\n",
    "\n",
    "        # Minimize the loss function\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target network using the soft update approach (slowly updating)\n",
    "        self.soft_update(self.critic_regular, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_regular, self.actor_target, TAU)\n",
    "\n",
    "\n",
    "    def soft_update(self, regular_model, target_model, tau):\n",
    "        # Update the target network slowly to improve the stability\n",
    "        for target_param, regular_param in zip(target_model.parameters(), regular_model.parameters()):\n",
    "            target_param.data.copy_(tau*regular_param.data + (1.0-tau) * target_param.data)\n",
    "            \n",
    "    def reset(self):\n",
    "        # Reset any necessary variables here, like noise parameters if you have.\n",
    "        pass\n",
    "\n",
    "# adding exploration noise to the actions can sometimes improve the speed of training in reinforcement learning environments, especially in the early stages where the agent needs to discover optimal strategies. The reason is that noise encourages exploration of the environment, which can help the agent to avoid getting stuck in sub-optimal policies.\n",
    "#In DDPG, an Ornstein-Uhlenbeck process is often used to generate temporally correlated exploration for exploration efficiency in physical control problems with inertia.\n",
    "'''\n",
    "class OUNoise:\n",
    "    def __init__(self, action_space_size, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.mu = mu\n",
    "        #elf.mu = mu * np.ones(action_space_size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        #self.seed = random.seed(seed)\n",
    "        #self.reset()\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        #self.state = copy.copy(self.mu)\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "\n",
    "    def get_noise(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "'''    \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "GOAL = 30.1\n",
    "SCORE_AVERAGED = 100\n",
    "PRINT_EVERY = 20\n",
    "N_EPISODES = 500\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "# reset the environment and extract state and action spaces\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# Initialize the agents\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, random_seed=0)\n",
    "\n",
    "#  Method for training the agent\n",
    "def train(n_episodes=N_EPISODES):\n",
    "    scores_deque = deque(maxlen=SCORE_AVERAGED)\n",
    "    scores = []\n",
    "    for i_episode in range(1, N_EPISODES+1):\n",
    "        #agent.noise.reset()\n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations\n",
    "        score_all_agents = np.zeros(num_agents) \n",
    "        for t in range(1, MAX_TIMESTEPS+1):\n",
    "            \n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name] \n",
    "            next_states = env_info.vector_observations \n",
    "            rewards = env_info.rewards                        \n",
    "            dones = env_info.local_done \n",
    "            ## Store experience of all the agents\n",
    "            for (state, action, reward, next_state, done) \\\n",
    "                    in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "            states = next_states\n",
    "            score_all_agents += rewards\n",
    "        \n",
    "        scores_deque.append(np.mean(score_all_agents))\n",
    "        scores.append(np.mean(score_all_agents))\n",
    "        \n",
    "        print('\\rEpisode {:3d} \\tScore: {:5.2f} \\t' \\\n",
    "              'Moving average: {:5.2f}' \\\n",
    "              .format(i_episode,  np.mean(score_all_agents), np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            torch.save(agent.actor_regular.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\r\\nEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque) >= GOAL:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                         np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_regular.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "# Train the agent\n",
    "scores = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def reset_parameters(layers):\n",
    "    for layer in layers:\n",
    "        layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_layers=[400,300]):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc_layers (list): Number of nodes in hidden layers\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Define input and output values for the hidden layers\n",
    "        dims = [state_size] + fc_layers + [action_size]\n",
    "        # Create the hidden layers\n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(dim_in, dim_out) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        # Initialize the hidden layer weights\n",
    "        reset_parameters(self.fc_layers)\n",
    "\n",
    "        #print('Actor network built:', self.fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        # Pass the input through all the layers apllying ReLU activation, but the last\n",
    "        for layer in self.fc_layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Pass the result through the output layer apllying hyperbolic tangent function\n",
    "        x = torch.tanh(self.fc_layers[-1](x))\n",
    "        # Return the better action for the input state\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_layers=[400,300]):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc_layers (list): Number of nodes in hidden layers\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Append the output size to the layers's dimensions\n",
    "        dims = fc_layers + [1]\n",
    "        # Create a list of layers\n",
    "        layers_list = []\n",
    "        layers_list.append(nn.Linear(state_size, dims[0]))\n",
    "        # The second layer receives the the first layer output + action\n",
    "        layers_list.append(nn.Linear(dims[0] + action_size, dims[1]))\n",
    "        # Build the next layers, if that is the case\n",
    "        for dim_in, dim_out in zip(dims[1:-1], dims[2:]):\n",
    "            layers_list.append(nn.Linear(dim_in, dim_out))\n",
    "        # Store the layers as a ModuleList\n",
    "        self.fc_layers = nn.ModuleList(layers_list)\n",
    "        # Initialize the hidden layer weights\n",
    "        reset_parameters(self.fc_layers)\n",
    "        # Add batch normalization to the first hidden layer\n",
    "        self.bn = nn.BatchNorm1d(dims[0])\n",
    "\n",
    "        #print('Critic network built:', self.fc_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        # Pass the states into the first layer\n",
    "        x = self.fc_layers[0](state)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        # Concatenate the first layer output with the action\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        # Pass the input through all the layers apllying ReLU activation, but the last\n",
    "        for layer in self.fc_layers[1:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Pass the result through the output layer apllying sigmoid activation\n",
    "        x = torch.sigmoid(self.fc_layers[-1](x))\n",
    "        # Return the Q-Value for the input state-action\n",
    "        return x\n",
    "\n",
    "# Replay Buffer Size\n",
    "BUFFER_SIZE = int(1e6)\n",
    "# Minibatch Size\n",
    "BATCH_SIZE = 256\n",
    "# Discount Gamma\n",
    "GAMMA = 0.995 \n",
    "# Soft Update Value\n",
    "TAU = 1e-2   \n",
    "# Learning rates for each NN      \n",
    "LR_ACTOR = 1e-3 \n",
    "LR_CRITIC = 1e-3\n",
    "# Update network every X timesteps\n",
    "UPDATE_EVERY = 32\n",
    "# Learn from batch of experiences n_experiences times\n",
    "N_EXPERIENCES = 16   \n",
    "\n",
    "NOISE_DECAY = 0.999\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment using the DDPG algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Neural Network (Regular and target)\n",
    "        self.actor_regular = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Neural Network (Regular and target)\n",
    "        self.critic_regular = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        self.noise_decay = NOISE_DECAY\n",
    "          \n",
    "        # Ensure that both networks have the same weights\n",
    "        self.soft_update(self.actor_target, self.actor_regular, TAU)\n",
    "        self.soft_update(self.critic_target, self.critic_regular, TAU)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        # Save collected experiences\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn from our buffer if possible\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\n",
    "            for _ in range(N_EXPERIENCES):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        # Evaluation mode\n",
    "        # Notify all your layers that you are in eval mode, that way, \n",
    "        # Batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "        self.actor_regular.eval()\n",
    "        # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "        # It will reduce memory usage and speed up\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_regular(states).cpu().data.numpy() + self.noise.get_noise()\n",
    "        # Enable Training mode\n",
    "        self.actor_regular.train()\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the critic neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Get predicted next-state actions\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        #Get Q values from target model\n",
    "        #Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        #Detach tensor from computation graph before performing operations on it:\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next).detach()\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Calculate the critic loss\n",
    "        Q_expected = self.critic_regular(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the actor neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Calculate the actor loss\n",
    "        actions_pred = self.actor_regular(states)\n",
    "        # Change sign because of the gradient descent\n",
    "        actor_loss = -self.critic_regular(states, actions_pred).mean()\n",
    "\n",
    "        # Minimize the loss function\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target network using the soft update approach (slowly updating)\n",
    "        self.soft_update(self.critic_regular, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_regular, self.actor_target, TAU)\n",
    "\n",
    "\n",
    "    def soft_update(self, regular_model, target_model, tau):\n",
    "        # Update the target network slowly to improve the stability\n",
    "        for target_param, regular_param in zip(target_model.parameters(), regular_model.parameters()):\n",
    "            target_param.data.copy_(tau*regular_param.data + (1.0-tau) * target_param.data)\n",
    "            \n",
    "    def reset(self):\n",
    "        # Reset any necessary variables here, like noise parameters if you have.\n",
    "        pass\n",
    "\n",
    "# adding exploration noise to the actions can sometimes improve the speed of training in reinforcement learning environments, especially in the early stages where the agent needs to discover optimal strategies. The reason is that noise encourages exploration of the environment, which can help the agent to avoid getting stuck in sub-optimal policies.\n",
    "#In DDPG, an Ornstein-Uhlenbeck process is often used to generate temporally correlated exploration for exploration efficiency in physical control problems with inertia.\n",
    "class OUNoise:\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(size=x.shape)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "GOAL = 30.1\n",
    "SCORE_AVERAGED = 100\n",
    "PRINT_EVERY = 10\n",
    "N_EPISODES = 500\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "# reset the environment and extract state and action spaces\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# Initialize the agents\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, random_seed=0)\n",
    "\n",
    "#  Method for training the agent\n",
    "def train(n_episodes=N_EPISODES):\n",
    "    scores_deque = deque(maxlen=SCORE_AVERAGED)\n",
    "    scores = []\n",
    "    for i_episode in range(1, N_EPISODES+1):\n",
    "        agent.noise.reset()\n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations\n",
    "        score_all_agents = np.zeros(num_agents) \n",
    "        for t in range(1, MAX_TIMESTEPS+1):\n",
    "            \n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name] \n",
    "            next_states = env_info.vector_observations \n",
    "            rewards = env_info.rewards                        \n",
    "            dones = env_info.local_done \n",
    "            ## Store experience of all the agents\n",
    "            for (state, action, reward, next_state, done) \\\n",
    "                    in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "            states = next_states\n",
    "            score_all_agents += rewards\n",
    "        \n",
    "        scores_deque.append(np.mean(score_all_agents))\n",
    "        scores.append(np.mean(score_all_agents))\n",
    "        \n",
    "        print('\\rEpisode {:3d} \\tScore: {:5.2f} \\t' \\\n",
    "              'Moving average: {:5.2f}' \\\n",
    "              .format(i_episode,  np.mean(score_all_agents), np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            torch.save(agent.actor_regular.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "            \n",
    "        if np.mean(scores_deque) >= GOAL:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                         np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_regular.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'checkpoint_critic.pth')\n",
    "            \n",
    "            break\n",
    "    return scores\n",
    "\n",
    "# Train the agent\n",
    "scores = train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot scores of trained agent\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def reset_parameters(layers):\n",
    "    for layer in layers:\n",
    "        layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_layers=[400,300]):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc_layers (list): Number of nodes in hidden layers\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Define input and output values for the hidden layers\n",
    "        dims = [state_size] + fc_layers + [action_size]\n",
    "        # Create the hidden layers\n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(dim_in, dim_out) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        # Initialize the hidden layer weights\n",
    "        reset_parameters(self.fc_layers)\n",
    "\n",
    "        #print('Actor network built:', self.fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        # Pass the input through all the layers apllying ReLU activation, but the last\n",
    "        for layer in self.fc_layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Pass the result through the output layer apllying hyperbolic tangent function\n",
    "        x = torch.tanh(self.fc_layers[-1](x))\n",
    "        # Return the better action for the input state\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_layers=[400,300]):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc_layers (list): Number of nodes in hidden layers\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Append the output size to the layers's dimensions\n",
    "        dims = fc_layers + [1]\n",
    "        # Create a list of layers\n",
    "        layers_list = []\n",
    "        layers_list.append(nn.Linear(state_size, dims[0]))\n",
    "        # The second layer receives the the first layer output + action\n",
    "        layers_list.append(nn.Linear(dims[0] + action_size, dims[1]))\n",
    "        # Build the next layers, if that is the case\n",
    "        for dim_in, dim_out in zip(dims[1:-1], dims[2:]):\n",
    "            layers_list.append(nn.Linear(dim_in, dim_out))\n",
    "        # Store the layers as a ModuleList\n",
    "        self.fc_layers = nn.ModuleList(layers_list)\n",
    "        # Initialize the hidden layer weights\n",
    "        reset_parameters(self.fc_layers)\n",
    "        # Add batch normalization to the first hidden layer\n",
    "        self.bn = nn.BatchNorm1d(dims[0])\n",
    "\n",
    "        #print('Critic network built:', self.fc_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        # Pass the states into the first layer\n",
    "        x = self.fc_layers[0](state)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        # Concatenate the first layer output with the action\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        # Pass the input through all the layers apllying ReLU activation, but the last\n",
    "        for layer in self.fc_layers[1:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Pass the result through the output layer apllying sigmoid activation\n",
    "        x = torch.sigmoid(self.fc_layers[-1](x))\n",
    "        # Return the Q-Value for the input state-action\n",
    "        return x\n",
    "\n",
    "# Replay Buffer Size\n",
    "BUFFER_SIZE = int(1e6)\n",
    "# Minibatch Size\n",
    "BATCH_SIZE = 256\n",
    "# Discount Gamma\n",
    "GAMMA = 0.995 \n",
    "# Soft Update Value\n",
    "TAU = 1e-2   \n",
    "# Learning rates for each NN      \n",
    "LR_ACTOR = 1e-3 \n",
    "LR_CRITIC = 1e-3\n",
    "# Update network every X timesteps\n",
    "UPDATE_EVERY = 32\n",
    "# Learn from batch of experiences n_experiences times\n",
    "N_EXPERIENCES = 16   \n",
    "\n",
    "#NOISE_DECAY = 0.999\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment using the DDPG algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Neural Network (Regular and target)\n",
    "        self.actor_regular = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Neural Network (Regular and target)\n",
    "        self.critic_regular = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        # Noise process\n",
    "        #self.noise = OUNoise(action_size, random_seed)\n",
    "        #self.noise_decay = NOISE_DECAY\n",
    "          \n",
    "        # Ensure that both networks have the same weights\n",
    "        self.soft_update(self.actor_target, self.actor_regular, TAU)\n",
    "        self.soft_update(self.critic_target, self.critic_regular, TAU)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        # Save collected experiences\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn from our buffer if possible\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\n",
    "            for _ in range(N_EXPERIENCES):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        # Evaluation mode\n",
    "        # Notify all your layers that you are in eval mode, that way, \n",
    "        # Batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "        self.actor_regular.eval()\n",
    "        # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "        # It will reduce memory usage and speed up\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_regular(states).cpu().data.numpy() #+ self.noise.get_noise()\n",
    "        # Enable Training mode\n",
    "        self.actor_regular.train()\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    #def reset(self):\n",
    "    #    self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the critic neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Get predicted next-state actions\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        #Get Q values from target model\n",
    "        #Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        #Detach tensor from computation graph before performing operations on it:\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next).detach()\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Calculate the critic loss\n",
    "        Q_expected = self.critic_regular(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the actor neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        # Calculate the actor loss\n",
    "        actions_pred = self.actor_regular(states)\n",
    "        # Change sign because of the gradient descent\n",
    "        actor_loss = -self.critic_regular(states, actions_pred).mean()\n",
    "\n",
    "        # Minimize the loss function\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target network using the soft update approach (slowly updating)\n",
    "        self.soft_update(self.critic_regular, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_regular, self.actor_target, TAU)\n",
    "\n",
    "\n",
    "    def soft_update(self, regular_model, target_model, tau):\n",
    "        # Update the target network slowly to improve the stability\n",
    "        for target_param, regular_param in zip(target_model.parameters(), regular_model.parameters()):\n",
    "            target_param.data.copy_(tau*regular_param.data + (1.0-tau) * target_param.data)\n",
    "            \n",
    "    def reset(self):\n",
    "        # Reset any necessary variables here, like noise parameters if you have.\n",
    "        pass\n",
    "\n",
    "# adding exploration noise to the actions can sometimes improve the speed of training in reinforcement learning environments, especially in the early stages where the agent needs to discover optimal strategies. The reason is that noise encourages exploration of the environment, which can help the agent to avoid getting stuck in sub-optimal policies.\n",
    "#In DDPG, an Ornstein-Uhlenbeck process is often used to generate temporally correlated exploration for exploration efficiency in physical control problems with inertia.\n",
    "'''\n",
    "class OUNoise:\n",
    "    def __init__(self, action_space_size, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.mu = mu\n",
    "        #elf.mu = mu * np.ones(action_space_size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        #self.seed = random.seed(seed)\n",
    "        #self.reset()\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        #self.state = copy.copy(self.mu)\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "\n",
    "    def get_noise(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "'''    \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "GOAL = 30.1\n",
    "SCORE_AVERAGED = 100\n",
    "PRINT_EVERY = 20\n",
    "N_EPISODES = 500\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "# reset the environment and extract state and action spaces\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# Initialize the agents\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, random_seed=0)\n",
    "\n",
    "#  Method for training the agent\n",
    "def train(n_episodes=N_EPISODES):\n",
    "    scores_deque = deque(maxlen=SCORE_AVERAGED)\n",
    "    scores = []\n",
    "    for i_episode in range(1, N_EPISODES+1):\n",
    "        #agent.noise.reset()\n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations\n",
    "        score_all_agents = np.zeros(num_agents) \n",
    "        for t in range(1, MAX_TIMESTEPS+1):\n",
    "            \n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name] \n",
    "            next_states = env_info.vector_observations \n",
    "            rewards = env_info.rewards                        \n",
    "            dones = env_info.local_done \n",
    "            ## Store experience of all the agents\n",
    "            for (state, action, reward, next_state, done) \\\n",
    "                    in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "            states = next_states\n",
    "            score_all_agents += rewards\n",
    "        \n",
    "        scores_deque.append(np.mean(score_all_agents))\n",
    "        scores.append(np.mean(score_all_agents))\n",
    "        \n",
    "        print('\\rEpisode {:3d} \\tScore: {:5.2f} \\t' \\\n",
    "              'Moving average: {:5.2f}' \\\n",
    "              .format(i_episode,  np.mean(score_all_agents), np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            torch.save(agent.actor_regular.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\r\\nEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque) >= GOAL:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                         np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_regular.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DDPGAgent(state_size, action_size, random_seed=0)\n",
    "agent.actor_regular.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_regular.load_state_dict(torch.load('checkpoint_critic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 25.855999422073364\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states)           ## select actions from DDPG policy\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                              # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points to Highlight:\n",
    "- Model Architecture\n",
    "- Training Process\n",
    "- Hyperparameters\n",
    "\n",
    "#### Model Architecture:\n",
    "\n",
    "Actor Network:\n",
    "\n",
    "- Receives states as input and produces continuous actions.\n",
    "- Employs fully connected layers with ReLU activation, except for the output layer which utilizes hyperbolic tangent activation to constrain action values within a defined range.\n",
    "- Batch normalization is not included in the actor network architecture.\n",
    "\n",
    "Critic Network:\n",
    "\n",
    "- Evaluates action quality based on both states and actions.\n",
    "- Utilizes fully connected layers with ReLU activation, culminating in a sigmoid activation for Q-value output.\n",
    "- Incorporates batch normalization in the first hidden layer to enhance stability during training.\n",
    "\n",
    "Shared Characteristics:\n",
    "\n",
    "- Both networks consist of two hidden layers with 400 and 300 units, respectively.\n",
    "- Weight initialization for all layers follows a uniform distribution within the range [-3e-3, 3e-3].\n",
    "- Both networks feature regular and target versions, with target networks updated gradually using a soft update approach.\n",
    "- PyTorch's ModuleList is utilized for dynamic layer management in both networks.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "- Replay Buffer: Experiences (state, action, reward, next_state, done) are stored and sampled randomly for training stability.\n",
    "\n",
    "- Learning: The agent learns from experiences sampled from the replay buffer. The critic minimizes the mean squared error between predicted and target Q-values, while the actor maximizes Q-values predicted by the critic for selected actions.\n",
    "\n",
    "#### Hyperparameters:\n",
    "- Buffer Size: 1e6\n",
    "- Batch Size: 256\n",
    "- Discount Factor (Gamma): 0.995\n",
    "- Soft Update (TAU): 1e-2\n",
    "- Learning Rates: 1e-3 for both actor and critic\n",
    "- Update Frequency: Every 32 timesteps\n",
    "- Number of Experiences: 16\n",
    "- Goal Score: 30.1\n",
    "- Print Frequency: Every 10 episodes\n",
    "- Max Timesteps per Episode: 1000\n",
    "- Number of Episodes: 500\n",
    "\n",
    "#### Future developments\n",
    "Noise: Noise code implementation (an Ornstein-Uhlenbeck process) is commented out but could improve exploration,  increasing the speed of learning, especially in the earliest stages (noise decay parameter)\n",
    "\n",
    "Target Network Updates: In learn() method of the DDPGAgent class, the updates of the target networks (actor_target and critic_target) happen every time learn() is called. However, the target networks should typically be updated less frequently to stabilize training. We could consider updating the target networks periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
