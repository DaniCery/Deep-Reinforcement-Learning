{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /data/Tennis_Linux_NoVis/Tennis.x86_64\n",
      "Mono path[0] = '/data/Tennis_Linux_NoVis/Tennis_Data/Managed'\n",
      "Mono config path = '/data/Tennis_Linux_NoVis/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "Logging to /home/student/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.004 MaxReward: 0.100.\n",
      "Episode 200\tAverage Score: 0.005 MaxReward: 0.100.\n",
      "Episode 300\tAverage Score: 0.030 MaxReward: 0.100.\n",
      "Episode 400\tAverage Score: 0.035 MaxReward: 0.200.\n",
      "Episode 500\tAverage Score: 0.029 MaxReward: 0.200.\n",
      "Episode 600\tAverage Score: 0.061 MaxReward: 0.200.\n",
      "Episode 700\tAverage Score: 0.057 MaxReward: 0.200.\n",
      "Episode 800\tAverage Score: 0.089 MaxReward: 0.300.\n",
      "Episode 900\tAverage Score: 0.103 MaxReward: 0.500.\n",
      "Episode 1000\tAverage Score: 0.156 MaxReward: 0.900.\n",
      "Episode 1100\tAverage Score: 0.207 MaxReward: 1.200.\n",
      "Episode 1200\tAverage Score: 0.215 MaxReward: 1.200.\n",
      "Episode 1300\tAverage Score: 0.268 MaxReward: 2.600.\n",
      "Episode 1400\tAverage Score: 0.365 MaxReward: 2.600.\n",
      "Episode 1500\tAverage Score: 0.470 MaxReward: 2.600.\n",
      "Episode 1600\tAverage Score: 0.284 MaxReward: 2.600.\n",
      "\n",
      "Environment solved in 1649 episodes!\tAverage Score: 0.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "GOAL = 0.51\n",
    "SCORE_AVERAGED = 100\n",
    "PRINT_EVERY = 100\n",
    "N_EPISODES = 3000\n",
    "MAX_TIMESTEPS = 1000\n",
    "BUFFER_SIZE = 12000\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.995\n",
    "TAU = 1e-3\n",
    "# Learning rates for each NN      \n",
    "LR_ACTOR = 1e-3\n",
    "LR_CRITIC = 1e-3\n",
    "UPDATE_EVERY = 2\n",
    "N_EXPERIENCES = 4\n",
    "\n",
    "def reset_parameters(layers):\n",
    "    for layer in layers:\n",
    "        layer.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, seed):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, output_size)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = torch.unsqueeze(state,0)\n",
    "            \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.tanh(self.fc5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset_parameters([self.fc1, self.fc2, self.fc3, self.fc4, self.fc5])\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, seed):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        x_state_action = torch.cat((states, actions), dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x_state_action))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset_parameters([self.fc1, self.fc2, self.fc3, self.fc4, self.fc5])\n",
    "\n",
    "class ActorCritic():\n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        critic_input_size = (state_size + action_size)*num_agents\n",
    "\n",
    "        self.actor_regular = Actor(state_size, action_size, seed).to(DEVICE)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(DEVICE)\n",
    "\n",
    "        self.critic_regular = Critic(critic_input_size, seed).to(DEVICE)\n",
    "        self.critic_target = Critic(critic_input_size, seed).to(DEVICE)\n",
    "\n",
    "class DDPG():\n",
    "\n",
    "    def __init__(self, agent_id, model, action_size, random_seed):\n",
    "        self.id = agent_id\n",
    "\n",
    "        # Actor Neural Network (Regular and target)\n",
    "        self.actor_regular = model.actor_regular\n",
    "        self.actor_target = model.actor_target\n",
    "        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        # Critic Neural Network (Regular and target)\n",
    "        self.critic_regular = model.critic_regular\n",
    "        self.critic_target = model.critic_target\n",
    "        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=LR_CRITIC)\n",
    "     \n",
    "        # Ensure that both networks have the same weights\n",
    "        self.deep_copy(self.actor_target, self.actor_regular)\n",
    "        self.deep_copy(self.critic_target, self.critic_regular)\n",
    "        \n",
    "        \n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "        self.actor_regular.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = self.actor_regular(states).cpu().data.numpy()\n",
    "        \n",
    "        self.actor_regular.train()\n",
    "\n",
    "        # Clip action to the right interval\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def learn(self, memory, agent_id, experiences, all_next_actions, all_actions):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        #--------------------------------\n",
    "        # Update the critic neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        agent_id = torch.tensor([agent_id]).to(DEVICE)\n",
    "        actions_next = torch.cat(all_next_actions, dim=1).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "            \n",
    "        Q_expected = self.critic_regular(states, actions)\n",
    "        # Compute Q targets for current states filtered by agent id\n",
    "        Q_targets = rewards.index_select(1, agent_id) + (GAMMA * Q_targets_next * (1 - dones.index_select(1, agent_id)))\n",
    "        \n",
    "        # Calculate the critic loss\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets.detach())\n",
    "        \n",
    "        # Minimize the loss\n",
    "        critic_loss.backward()\n",
    "        \n",
    "        # Critic gradient clipping to 1\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_regular.parameters(), 1) \n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        #--------------------------------\n",
    "        # Update the actor neural network\n",
    "        #--------------------------------\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        # Detach actions of other agents\n",
    "        actions_pred = [actions if i == self.id else actions.detach() for i, actions in enumerate(all_actions)]\n",
    "        actions_pred = torch.cat(actions_pred, dim=1).to(DEVICE)\n",
    "        actor_loss = -self.critic_regular(states, actions_pred).mean()\n",
    "        \n",
    "        # Minimize the loss function\n",
    "        actor_loss.backward()\n",
    "\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target network using the soft update approach (slowly updating)\n",
    "        self.soft_update(self.critic_regular, self.critic_target)\n",
    "        self.soft_update(self.actor_regular, self.actor_target)\n",
    "        \n",
    "        \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        # Update the target network slowly to improve the stability\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(TAU*local_param.data + (1.0-TAU) * target_param.data)\n",
    "\n",
    "    def deep_copy(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "class MADDPG():\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.random_seed = random_seed\n",
    "        self.num_agents = num_agents\n",
    "        self.timestep_counter = 0\n",
    "        self.agents = self.setup_agents()\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def setup_agents(self):\n",
    "        agents = []\n",
    "        for i in range(self.num_agents):\n",
    "            model = ActorCritic(\n",
    "                num_agents=self.num_agents, state_size=self.state_size, action_size=self.action_size, seed=self.random_seed\n",
    "            )\n",
    "            agents.append(DDPG(i, model, self.action_size, self.random_seed))\n",
    "        return agents\n",
    "        \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        # Flat states and next states\n",
    "        states = states.reshape(1, -1)\n",
    "        next_states = next_states.reshape(1, -1)\n",
    "        # Add experience to the buffer\n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        self.timestep_counter = self.timestep_counter + 1 \n",
    "\n",
    "        # Learn from our buffer if possible\n",
    "        if len(self.memory) > BATCH_SIZE and self.timestep_counter % UPDATE_EVERY == 0:\n",
    "            # Sample experiences for each agent\n",
    "            for _ in range(N_EXPERIENCES):\n",
    "                experiences = [self.memory.sample() for _ in range(self.num_agents)]\n",
    "                self.learn(experiences)\n",
    "                \n",
    "    def act(self, states):\n",
    "        actions = []\n",
    "        for agent, state in zip(self.agents, states):\n",
    "            action = agent.act(state)  # No noise exploration\n",
    "            actions.append(action)\n",
    "                \n",
    "        # Return flattened actions\n",
    "        return np.array(actions).reshape(1, -1)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        next_actions = []\n",
    "        actions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            states, _ , _ , next_states, _ = experiences[i]\n",
    "            agent_id = torch.tensor([i]).to(DEVICE)            \n",
    "            \n",
    "            state = states.reshape(-1, self.action_size, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "            action = agent.actor_regular(state)\n",
    "            actions.append(action)\n",
    "\n",
    "            next_state = next_states.reshape(-1, self.action_size, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "            next_action = agent.actor_target(next_state)\n",
    "            next_actions.append(next_action)\n",
    "                       \n",
    "        # Call to the method learn for each agent using\n",
    "        # the related experiences and all actions/next actions\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            agent.learn(self.memory, i, experiences[i], next_actions, actions)\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(DEVICE)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "# Init env\n",
    "agent = MADDPG(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=89)\n",
    "    \n",
    "def train(agent, n_episodes=N_EPISODES):\n",
    "    global_scores = []\n",
    "    averaged_scores = []\n",
    "    scores_deque = deque(maxlen=SCORE_AVERAGED)\n",
    "    max_reward = 0.0\n",
    "    \n",
    "    for episode in range(1, N_EPISODES + 1):\n",
    "\n",
    "        episode_rewards = []\n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations # current states for each agent\n",
    "\n",
    "        for t in range(MAX_TIMESTEPS):\n",
    "            actions = agent.act(states)  # Act according to our policy\n",
    "\n",
    "            env_info = env.step(actions)[brain_name]   # Send the decided actions to all the agents\n",
    "\n",
    "            next_states = env_info.vector_observations # Get next state for each agent \n",
    "\n",
    "            rewards = env_info.rewards  # Get rewards obtained from each agent\n",
    "\n",
    "            dones = env_info.local_done # Info about if an env is done   \n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, dones) # Learn from the collected experience\n",
    "\n",
    "            states = next_states # Update current states   \n",
    "            episode_rewards.append(rewards) # Add the rewards received \n",
    "\n",
    "            # Stop the loop if an agent is done               \n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        # add up the rewards for each agent and get the max of these 2 agents scores.\n",
    "        episode_reward = np.max(np.sum(np.array(episode_rewards), axis=0))\n",
    "        global_scores.append(episode_reward) \n",
    "        scores_deque.append(episode_reward) \n",
    "        avg_score = np.mean(scores_deque)\n",
    "        averaged_scores.append(avg_score)\n",
    "        \n",
    "        if episode_reward > max_reward:\n",
    "            max_reward = episode_reward\n",
    "            \n",
    "        if episode % PRINT_EVERY == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.3f} MaxReward: {:.3f}.'.format(episode, avg_score, max_reward))\n",
    "            \n",
    "        if avg_score >= GOAL:  \n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, avg_score))\n",
    "            for i, agent in enumerate(agent.agents):\n",
    "                torch.save(agent.actor_regular.state_dict(), f'checkpoint_actor_agent_v2_{i}.pth')\n",
    "                torch.save(agent.critic_regular.state_dict(), f'checkpoint_critic_agent_v2_{i}.pth')\n",
    "            break\n",
    "            \n",
    "    return global_scores, averaged_scores\n",
    "\n",
    "# Train agent and get results\n",
    "scores, averages = train(agent)\n",
    "\n",
    "# Plot Scores\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.arange(1, len(scores) + 1), averages)\n",
    "plt.ylabel('Tennis Environment Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL IMPLEMENTATION\n",
    "\n",
    "#### Actor-Critic Network Architecture\n",
    "\n",
    "##### Actor Network: \n",
    "The Actor network is responsible for selecting actions based on the current state of the agent. It maps the state space to the action space through a series of fully connected layers with batch normalization and activation functions to enhance learning stability and performance.\n",
    "\n",
    "- Input Layer: Linear layer with input size equal to the state dimension (state vector consisting of 24 continuous variables representing agent's velocity, the position of the ball, ecc..\n",
    "- Hidden Layers:\n",
    "    - fc1: Fully connected layer with 128 units, followed by ReLU activation.\n",
    "    - bn1: Batch normalization applied to the output of fc1.\n",
    "    - fc2: Fully connected layer with 64 units, followed by ReLU activation.\n",
    "    - fc3: Fully connected layer with 32 units, followed by ReLU activation.\n",
    "    - fc4: Fully connected layer with 16 units, followed by ReLU activation.\n",
    "- Output Layer: fc5 is a linear layer with the output size equal to the action dimension (vector of size 2. i.e. Moving the racket and hitting the ball with varying force/direction), followed by a tanh activation to scale actions to the range [-1, 1].\n",
    "\n",
    "##### Critic Network\n",
    "\n",
    "The Critic network evaluates the Q-value of a state-action pair, providing a scalar value representing the expected return. It combines both state and action inputs and processes them through several layers.\n",
    "\n",
    "- Input Layer: Linear layer with input size equal to the combined dimensions of the state and action spaces.\n",
    "- Hidden Layers:\n",
    "    - fc1: Fully connected layer with 256 units, followed by ReLU activation.\n",
    "    - bn1: Batch normalization applied to the output of fc1.\n",
    "    - fc2: Fully connected layer with 128 units, followed by ReLU activation.\n",
    "    - fc3: Fully connected layer with 64 units, followed by ReLU activation.\n",
    "    - fc4: Fully connected layer with 32 units, followed by ReLU activation.\n",
    "- Output Layer: fc5 is a linear layer with a single output representing the Q-value.\n",
    "\n",
    "#### Algorithm\n",
    "The MADDPG algorithm extends the DDPG algorithm to multiple agents. Each agent has its own Actor and Critic networks but shares the same replay buffer. The main components of the algorithm are:\n",
    "\n",
    "- Experience Replay: Experiences are stored in a replay buffer and sampled randomly to break the temporal correlations between experiences.\n",
    "- Soft Updates: The target networks (Actor and Critic) are updated using soft updates to improve stability.\n",
    "- Critic Update: The Critic network is updated by minimizing the mean squared error loss between the predicted Q-values and the target Q-values.\n",
    "- Actor Update: The Actor network is updated by maximizing the expected Q-value as determined by the Critic network.\n",
    "- MADDPG Wrapper: The MADDPG wrapper handles multiple DDPG agents and coordinates their interactions and learning processes. Components:\n",
    "\n",
    "#### Hyperparameters\n",
    "- Actor: 1e-3\n",
    "- Critic: 1e-3\n",
    "- Replay Buffer Size: 12000\n",
    "- Batch Size: 256\n",
    "- Discount Factor (Gamma): 0.995\n",
    "- Soft Update Parameter (Tau): 1e-3\n",
    "- Update Frequency: Every 2 timesteps\n",
    "- Number of Experiences: 4 experiences per update cycle\n",
    "\n",
    "#### Training\n",
    "The training process is evaluated based on the average score and the highest reward obtained during the episodes. The training loop prints out progress every 100 episodes and saves the model weights if the goal is achieved.\n",
    "\n",
    "### CONCLUSIONS\n",
    "The use of Actor-Critic networks, experience replay, and soft updates enables the agents to learn cooperative policies effectively, achieving the desired goal of an average score of at least 0.51.\n",
    "\n",
    "### FUTURE UPGRADES\n",
    "Noise could also be implemented to improve stability of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
